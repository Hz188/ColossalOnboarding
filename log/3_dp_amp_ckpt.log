[2024-05-07 07:03:54,023] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-05-07 07:03:54,023] torch.distributed.run: [WARNING] 
[2024-05-07 07:03:54,023] torch.distributed.run: [WARNING] *****************************************
[2024-05-07 07:03:54,023] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-07 07:03:54,023] torch.distributed.run: [WARNING] *****************************************
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-199]:53057 (errno: 22 - Invalid argument).
====================initialize the distributed env====================
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.88s/it]
====================get the model with lora====================
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.59s/it]
====================Use auto mixed precision training====================
====================Use gradient checkpoint====================
====================Use distributed data parallel====================
====================get the dataset====================
====================get the dataloader====================
====================start train====================
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Step: 1	 Data: torch.Size([10, 384])	 Training Loss: 1.0071955919265747
Step: 2	 Data: torch.Size([10, 384])	 Training Loss: 1.0457985401153564
Step: 3	 Data: torch.Size([10, 384])	 Training Loss: 1.1256227493286133
Step: 4	 Data: torch.Size([10, 384])	 Training Loss: 1.0782827138900757
Step: 5	 Data: torch.Size([10, 384])	 Training Loss: 1.0081820487976074
Step: 6	 Data: torch.Size([10, 384])	 Training Loss: 1.144866704940796
Step: 7	 Data: torch.Size([10, 384])	 Training Loss: 1.0281987190246582
Step: 8	 Data: torch.Size([10, 384])	 Training Loss: 1.2214652299880981
Step: 9	 Data: torch.Size([10, 384])	 Training Loss: 1.083243727684021
Step: 10	 Data: torch.Size([10, 384])	 Training Loss: 1.0878078937530518
Step: 11	 Data: torch.Size([10, 384])	 Training Loss: 1.0145107507705688
Step: 12	 Data: torch.Size([10, 384])	 Training Loss: 1.0510433912277222
Step: 13	 Data: torch.Size([10, 384])	 Training Loss: 1.0900883674621582
Step: 14	 Data: torch.Size([10, 384])	 Training Loss: 1.2331277132034302
Step: 15	 Data: torch.Size([10, 384])	 Training Loss: 1.038330316543579
Step: 16	 Data: torch.Size([10, 384])	 Training Loss: 1.126283049583435
Step: 17	 Data: torch.Size([10, 384])	 Training Loss: 1.1557906866073608
Step: 18	 Data: torch.Size([10, 384])	 Training Loss: 1.0550756454467773
Step: 19	 Data: torch.Size([10, 384])	 Training Loss: 1.2150639295578003
Step: 20	 Data: torch.Size([10, 384])	 Training Loss: 1.0172839164733887
Step: 21	 Data: torch.Size([10, 384])	 Training Loss: 1.0248085260391235
Step: 22	 Data: torch.Size([10, 384])	 Training Loss: 1.0429673194885254
Step: 23	 Data: torch.Size([10, 384])	 Training Loss: 1.136165976524353
Step: 24	 Data: torch.Size([10, 384])	 Training Loss: 1.1659212112426758
Step: 25	 Data: torch.Size([10, 384])	 Training Loss: 0.9777047038078308
Step: 26	 Data: torch.Size([10, 384])	 Training Loss: 0.9541589617729187
Step: 27	 Data: torch.Size([10, 384])	 Training Loss: 1.2132644653320312
Step: 28	 Data: torch.Size([10, 384])	 Training Loss: 1.106542706489563
Step: 29	 Data: torch.Size([10, 384])	 Training Loss: 1.111446499824524
Step: 30	 Data: torch.Size([10, 384])	 Training Loss: 0.9730271697044373
Step: 31	 Data: torch.Size([10, 384])	 Training Loss: 1.114084243774414
Step: 32	 Data: torch.Size([10, 384])	 Training Loss: 1.1126095056533813
Step: 33	 Data: torch.Size([10, 384])	 Training Loss: 1.0188575983047485
Step: 34	 Data: torch.Size([10, 384])	 Training Loss: 0.8922017812728882
Step: 35	 Data: torch.Size([10, 384])	 Training Loss: 1.0242365598678589
Step: 36	 Data: torch.Size([10, 384])	 Training Loss: 1.0513806343078613
Step: 37	 Data: torch.Size([10, 384])	 Training Loss: 1.099105954170227
Step: 38	 Data: torch.Size([10, 384])	 Training Loss: 0.9515454769134521
Step: 39	 Data: torch.Size([10, 384])	 Training Loss: 0.9978280663490295
Step: 40	 Data: torch.Size([10, 384])	 Training Loss: 1.0245283842086792
Step: 41	 Data: torch.Size([10, 384])	 Training Loss: 1.1570971012115479
Step: 42	 Data: torch.Size([10, 384])	 Training Loss: 0.9996838569641113
Step: 43	 Data: torch.Size([10, 384])	 Training Loss: 1.0843052864074707
Step: 44	 Data: torch.Size([10, 384])	 Training Loss: 1.2343943119049072
Step: 45	 Data: torch.Size([10, 384])	 Training Loss: 1.0849589109420776
Step: 46	 Data: torch.Size([10, 384])	 Training Loss: 0.9638962149620056
Step: 47	 Data: torch.Size([10, 384])	 Training Loss: 0.9677740335464478
Step: 48	 Data: torch.Size([10, 384])	 Training Loss: 1.194435954093933
Step: 49	 Data: torch.Size([10, 384])	 Training Loss: 1.1375998258590698
Step: 50	 Data: torch.Size([10, 384])	 Training Loss: 1.190363883972168
Step: 51	 Data: torch.Size([10, 384])	 Training Loss: 1.0037610530853271
Step: 52	 Data: torch.Size([10, 384])	 Training Loss: 1.0082595348358154
Step: 53	 Data: torch.Size([10, 384])	 Training Loss: 1.0834877490997314
Step: 54	 Data: torch.Size([10, 384])	 Training Loss: 1.065086841583252
Step: 55	 Data: torch.Size([10, 384])	 Training Loss: 0.9692537784576416
Step: 56	 Data: torch.Size([10, 384])	 Training Loss: 1.1094188690185547
Step: 57	 Data: torch.Size([10, 384])	 Training Loss: 0.9633925557136536
Step: 58	 Data: torch.Size([10, 384])	 Training Loss: 0.8926497101783752
Step: 59	 Data: torch.Size([10, 384])	 Training Loss: 1.122241497039795
Step: 60	 Data: torch.Size([10, 384])	 Training Loss: 1.1114662885665894
Step: 61	 Data: torch.Size([10, 384])	 Training Loss: 0.9478691220283508
Step: 62	 Data: torch.Size([10, 384])	 Training Loss: 1.1828233003616333
Step: 63	 Data: torch.Size([10, 384])	 Training Loss: 1.0636255741119385
Step: 64	 Data: torch.Size([10, 384])	 Training Loss: 0.9969176650047302
Step: 65	 Data: torch.Size([10, 384])	 Training Loss: 0.9889199137687683
Step: 66	 Data: torch.Size([10, 384])	 Training Loss: 1.0143835544586182
Step: 67	 Data: torch.Size([10, 384])	 Training Loss: 1.01730215549469
Step: 68	 Data: torch.Size([10, 384])	 Training Loss: 1.150455117225647
Step: 69	 Data: torch.Size([10, 384])	 Training Loss: 1.1764601469039917
Step: 70	 Data: torch.Size([10, 384])	 Training Loss: 0.9328269362449646
Step: 71	 Data: torch.Size([10, 384])	 Training Loss: 0.9493771195411682
Step: 72	 Data: torch.Size([10, 350])	 Training Loss: 1.4320223331451416
Step: 73	 Data: torch.Size([10, 384])	 Training Loss: 0.9332274198532104
Step: 74	 Data: torch.Size([10, 384])	 Training Loss: 1.0684537887573242
Step: 75	 Data: torch.Size([10, 384])	 Training Loss: 0.9829528331756592
Step: 76	 Data: torch.Size([10, 384])	 Training Loss: 0.8672439455986023
Step: 77	 Data: torch.Size([10, 384])	 Training Loss: 1.1169452667236328
Step: 78	 Data: torch.Size([10, 384])	 Training Loss: 1.0293201208114624
Step: 79	 Data: torch.Size([10, 384])	 Training Loss: 1.2365894317626953
Step: 80	 Data: torch.Size([10, 384])	 Training Loss: 1.0641180276870728
Step: 81	 Data: torch.Size([10, 384])	 Training Loss: 1.03739595413208
Step: 82	 Data: torch.Size([10, 384])	 Training Loss: 1.065831184387207
Step: 83	 Data: torch.Size([10, 384])	 Training Loss: 1.0419877767562866
Step: 84	 Data: torch.Size([10, 384])	 Training Loss: 0.86430424451828
Step: 85	 Data: torch.Size([10, 384])	 Training Loss: 0.9697842001914978
Step: 86	 Data: torch.Size([10, 384])	 Training Loss: 1.1055209636688232
Step: 87	 Data: torch.Size([10, 384])	 Training Loss: 1.0258829593658447
Step: 88	 Data: torch.Size([10, 384])	 Training Loss: 0.8821370601654053
Step: 89	 Data: torch.Size([10, 384])	 Training Loss: 1.077681303024292
Step: 90	 Data: torch.Size([10, 384])	 Training Loss: 1.0505316257476807
Step: 91	 Data: torch.Size([10, 384])	 Training Loss: 0.9035326242446899
Step: 92	 Data: torch.Size([10, 384])	 Training Loss: 0.8864270448684692
Step: 93	 Data: torch.Size([10, 384])	 Training Loss: 1.0007522106170654
Step: 94	 Data: torch.Size([10, 384])	 Training Loss: 1.099865436553955
Step: 95	 Data: torch.Size([10, 384])	 Training Loss: 0.9578977227210999
Step: 96	 Data: torch.Size([10, 384])	 Training Loss: 0.8947387337684631
Step: 97	 Data: torch.Size([10, 384])	 Training Loss: 1.0521023273468018
Step: 98	 Data: torch.Size([10, 384])	 Training Loss: 0.913997232913971
Step: 99	 Data: torch.Size([10, 384])	 Training Loss: 0.9818572998046875
Step: 100	 Data: torch.Size([10, 384])	 Training Loss: 1.1417336463928223
Step: 101	 Data: torch.Size([10, 384])	 Training Loss: 0.9626395106315613
Step: 102	 Data: torch.Size([10, 384])	 Training Loss: 0.9711547493934631
Step: 103	 Data: torch.Size([10, 384])	 Training Loss: 0.9994722604751587
Step: 104	 Data: torch.Size([10, 384])	 Training Loss: 0.938688337802887
Step: 105	 Data: torch.Size([10, 384])	 Training Loss: 0.984920859336853
Step: 106	 Data: torch.Size([10, 384])	 Training Loss: 1.1139508485794067
Step: 107	 Data: torch.Size([10, 384])	 Training Loss: 0.9333313703536987
Step: 108	 Data: torch.Size([10, 384])	 Training Loss: 0.9083548784255981
Step: 109	 Data: torch.Size([10, 384])	 Training Loss: 1.0009419918060303
Step: 110	 Data: torch.Size([10, 384])	 Training Loss: 0.9143251776695251
Step: 111	 Data: torch.Size([10, 384])	 Training Loss: 1.2004249095916748
Step: 112	 Data: torch.Size([10, 384])	 Training Loss: 1.1965830326080322
Step: 113	 Data: torch.Size([10, 384])	 Training Loss: 1.0623688697814941
Step: 114	 Data: torch.Size([10, 384])	 Training Loss: 0.9734879732131958
Step: 115	 Data: torch.Size([10, 384])	 Training Loss: 0.8932327032089233
Step: 116	 Data: torch.Size([10, 384])	 Training Loss: 0.9169145822525024
Step: 117	 Data: torch.Size([10, 384])	 Training Loss: 0.8966324329376221
Step: 118	 Data: torch.Size([10, 384])	 Training Loss: 0.9382227659225464
Step: 119	 Data: torch.Size([10, 384])	 Training Loss: 1.3219324350357056
Step: 120	 Data: torch.Size([10, 384])	 Training Loss: 0.8725711107254028
Step: 121	 Data: torch.Size([10, 307])	 Training Loss: 1.0819509029388428
Step: 122	 Data: torch.Size([10, 384])	 Training Loss: 1.1324694156646729
Step: 123	 Data: torch.Size([10, 384])	 Training Loss: 0.9771714806556702
Step: 124	 Data: torch.Size([10, 384])	 Training Loss: 0.8718648552894592
Step: 125	 Data: torch.Size([10, 384])	 Training Loss: 0.8889573216438293
Step: 126	 Data: torch.Size([10, 384])	 Training Loss: 0.9152998328208923
Step: 127	 Data: torch.Size([10, 384])	 Training Loss: 1.0413416624069214
Step: 128	 Data: torch.Size([10, 384])	 Training Loss: 0.8391543626785278
Step: 129	 Data: torch.Size([10, 384])	 Training Loss: 1.0830121040344238
Step: 130	 Data: torch.Size([10, 384])	 Training Loss: 0.9724349975585938
Step: 131	 Data: torch.Size([10, 384])	 Training Loss: 0.968781054019928
Step: 132	 Data: torch.Size([10, 384])	 Training Loss: 0.9563863277435303
Step: 133	 Data: torch.Size([10, 384])	 Training Loss: 0.9977688193321228
Step: 134	 Data: torch.Size([10, 384])	 Training Loss: 1.0640555620193481
Step: 135	 Data: torch.Size([10, 384])	 Training Loss: 0.9793288111686707
Step: 136	 Data: torch.Size([10, 384])	 Training Loss: 0.9313264489173889
Step: 137	 Data: torch.Size([10, 384])	 Training Loss: 1.0560463666915894
Step: 138	 Data: torch.Size([10, 384])	 Training Loss: 0.9172187447547913
Step: 139	 Data: torch.Size([10, 384])	 Training Loss: 1.0035650730133057
Step: 140	 Data: torch.Size([10, 384])	 Training Loss: 0.9589694738388062
Step: 141	 Data: torch.Size([10, 384])	 Training Loss: 0.985288143157959
Step: 142	 Data: torch.Size([10, 384])	 Training Loss: 0.9701365828514099
Step: 143	 Data: torch.Size([10, 384])	 Training Loss: 1.0093579292297363
Step: 144	 Data: torch.Size([10, 384])	 Training Loss: 1.0097352266311646
Step: 145	 Data: torch.Size([10, 384])	 Training Loss: 0.948100745677948
Step: 146	 Data: torch.Size([10, 384])	 Training Loss: 1.0057729482650757
Step: 147	 Data: torch.Size([10, 384])	 Training Loss: 0.9852777123451233
Step: 148	 Data: torch.Size([10, 384])	 Training Loss: 0.9345636367797852
Step: 149	 Data: torch.Size([10, 384])	 Training Loss: 0.8999558687210083
Step: 150	 Data: torch.Size([10, 384])	 Training Loss: 0.9691724181175232
Step: 151	 Data: torch.Size([10, 384])	 Training Loss: 1.0549875497817993
Step: 152	 Data: torch.Size([10, 384])	 Training Loss: 0.895561933517456
Step: 153	 Data: torch.Size([10, 384])	 Training Loss: 1.1105875968933105
Step: 154	 Data: torch.Size([10, 384])	 Training Loss: 1.0255658626556396
Step: 155	 Data: torch.Size([10, 384])	 Training Loss: 0.9326496720314026
Step: 156	 Data: torch.Size([10, 384])	 Training Loss: 0.840064287185669
Step: 157	 Data: torch.Size([10, 384])	 Training Loss: 0.8603005409240723
Step: 158	 Data: torch.Size([10, 384])	 Training Loss: 0.9801412224769592
Step: 159	 Data: torch.Size([10, 384])	 Training Loss: 1.0543982982635498
Step: 160	 Data: torch.Size([10, 384])	 Training Loss: 0.8694239854812622
Step: 161	 Data: torch.Size([10, 384])	 Training Loss: 1.0355725288391113
Step: 162	 Data: torch.Size([10, 384])	 Training Loss: 1.001602053642273
Step: 163	 Data: torch.Size([10, 384])	 Training Loss: 0.9838359951972961
Step: 164	 Data: torch.Size([10, 384])	 Training Loss: 0.9500579237937927
Step: 165	 Data: torch.Size([10, 384])	 Training Loss: 0.905616044998169
Step: 166	 Data: torch.Size([10, 384])	 Training Loss: 1.0810186862945557
Step: 167	 Data: torch.Size([10, 384])	 Training Loss: 0.9244688153266907
Step: 168	 Data: torch.Size([10, 384])	 Training Loss: 1.0217911005020142
Step: 169	 Data: torch.Size([10, 384])	 Training Loss: 0.9647475481033325
Step: 170	 Data: torch.Size([10, 384])	 Training Loss: 1.0723620653152466
Step: 171	 Data: torch.Size([10, 384])	 Training Loss: 0.9119941592216492
Step: 172	 Data: torch.Size([10, 384])	 Training Loss: 0.8488999605178833
Step: 173	 Data: torch.Size([10, 384])	 Training Loss: 0.9163545966148376
Step: 174	 Data: torch.Size([10, 384])	 Training Loss: 0.9024068713188171
Step: 175	 Data: torch.Size([10, 384])	 Training Loss: 1.0383535623550415
Step: 176	 Data: torch.Size([10, 384])	 Training Loss: 0.9513483643531799
Step: 177	 Data: torch.Size([10, 384])	 Training Loss: 0.992771565914154
Step: 178	 Data: torch.Size([10, 384])	 Training Loss: 0.9013361930847168
Step: 179	 Data: torch.Size([10, 384])	 Training Loss: 0.8368240594863892
Step: 180	 Data: torch.Size([10, 384])	 Training Loss: 1.0485342741012573
Step: 181	 Data: torch.Size([10, 384])	 Training Loss: 0.737377941608429
Step: 182	 Data: torch.Size([10, 384])	 Training Loss: 1.0804461240768433
Step: 183	 Data: torch.Size([10, 384])	 Training Loss: 0.8650123476982117
Step: 184	 Data: torch.Size([10, 384])	 Training Loss: 0.949315071105957
Step: 185	 Data: torch.Size([10, 384])	 Training Loss: 0.8506709337234497
Step: 186	 Data: torch.Size([10, 384])	 Training Loss: 0.9868221282958984
Step: 187	 Data: torch.Size([10, 384])	 Training Loss: 0.8678044080734253
Step: 188	 Data: torch.Size([10, 384])	 Training Loss: 0.9689838290214539
Step: 189	 Data: torch.Size([10, 384])	 Training Loss: 0.929105281829834
Step: 190	 Data: torch.Size([10, 384])	 Training Loss: 1.0986528396606445
Step: 191	 Data: torch.Size([10, 384])	 Training Loss: 0.890651285648346
Step: 192	 Data: torch.Size([10, 384])	 Training Loss: 1.0065467357635498
Step: 193	 Data: torch.Size([10, 384])	 Training Loss: 0.936141312122345
Step: 194	 Data: torch.Size([10, 384])	 Training Loss: 0.8804516196250916
Step: 195	 Data: torch.Size([10, 384])	 Training Loss: 0.9396939277648926
Step: 196	 Data: torch.Size([10, 384])	 Training Loss: 0.9160753488540649
Step: 197	 Data: torch.Size([10, 384])	 Training Loss: 0.8394050002098083
Step: 198	 Data: torch.Size([10, 384])	 Training Loss: 1.0161526203155518
Step: 199	 Data: torch.Size([10, 384])	 Training Loss: 1.0057599544525146
Step: 200	 Data: torch.Size([10, 384])	 Training Loss: 0.904188334941864
Step: 201	 Data: torch.Size([10, 384])	 Training Loss: 0.9712127447128296
Step: 202	 Data: torch.Size([10, 384])	 Training Loss: 0.9007289409637451
Step: 203	 Data: torch.Size([10, 384])	 Training Loss: 1.0082815885543823
Step: 204	 Data: torch.Size([10, 384])	 Training Loss: 0.9774750471115112
Step: 205	 Data: torch.Size([10, 384])	 Training Loss: 0.9956930875778198
Step: 206	 Data: torch.Size([10, 384])	 Training Loss: 0.8995668888092041
Step: 207	 Data: torch.Size([10, 384])	 Training Loss: 0.874534010887146
Step: 208	 Data: torch.Size([10, 384])	 Training Loss: 0.9840685725212097
Step: 209	 Data: torch.Size([10, 384])	 Training Loss: 0.9531558156013489
Step: 210	 Data: torch.Size([10, 384])	 Training Loss: 0.9881297945976257
Step: 211	 Data: torch.Size([10, 384])	 Training Loss: 0.8944720029830933
Step: 212	 Data: torch.Size([10, 384])	 Training Loss: 0.8839694857597351
Step: 213	 Data: torch.Size([10, 384])	 Training Loss: 0.9289335608482361
Step: 214	 Data: torch.Size([10, 384])	 Training Loss: 1.0806525945663452
Step: 215	 Data: torch.Size([10, 384])	 Training Loss: 0.9047776460647583
Step: 216	 Data: torch.Size([10, 384])	 Training Loss: 0.9155776500701904
Step: 217	 Data: torch.Size([10, 384])	 Training Loss: 1.0369081497192383
Step: 218	 Data: torch.Size([10, 384])	 Training Loss: 0.9831531643867493
Step: 219	 Data: torch.Size([10, 384])	 Training Loss: 1.0452536344528198
Step: 220	 Data: torch.Size([10, 384])	 Training Loss: 0.8752466440200806
Step: 221	 Data: torch.Size([10, 384])	 Training Loss: 1.0104954242706299
Step: 222	 Data: torch.Size([10, 384])	 Training Loss: 0.8537700772285461
Step: 223	 Data: torch.Size([10, 384])	 Training Loss: 0.9306738376617432
Step: 224	 Data: torch.Size([10, 384])	 Training Loss: 0.9318071007728577
Step: 225	 Data: torch.Size([10, 384])	 Training Loss: 0.814300537109375
Step: 226	 Data: torch.Size([10, 384])	 Training Loss: 0.8578932881355286
Step: 227	 Data: torch.Size([10, 384])	 Training Loss: 0.9830324053764343
Step: 228	 Data: torch.Size([10, 384])	 Training Loss: 0.907576322555542
Step: 229	 Data: torch.Size([10, 384])	 Training Loss: 0.896291971206665
Step: 230	 Data: torch.Size([10, 384])	 Training Loss: 0.8233621120452881
Step: 231	 Data: torch.Size([10, 384])	 Training Loss: 0.944464921951294
Step: 232	 Data: torch.Size([10, 384])	 Training Loss: 0.9850987792015076
Step: 233	 Data: torch.Size([10, 384])	 Training Loss: 0.9728972315788269
Step: 234	 Data: torch.Size([10, 384])	 Training Loss: 0.8221765756607056
Step: 235	 Data: torch.Size([10, 384])	 Training Loss: 0.8219953179359436
Step: 236	 Data: torch.Size([10, 384])	 Training Loss: 0.9466804265975952
Step: 237	 Data: torch.Size([10, 384])	 Training Loss: 1.023829460144043
Step: 238	 Data: torch.Size([10, 384])	 Training Loss: 1.0359697341918945
Step: 239	 Data: torch.Size([10, 384])	 Training Loss: 0.9324953556060791
Step: 240	 Data: torch.Size([10, 384])	 Training Loss: 0.946472704410553
Step: 241	 Data: torch.Size([10, 384])	 Training Loss: 1.0216840505599976
Step: 242	 Data: torch.Size([10, 384])	 Training Loss: 0.9068096876144409
Step: 243	 Data: torch.Size([10, 384])	 Training Loss: 1.0612223148345947
Step: 244	 Data: torch.Size([10, 384])	 Training Loss: 1.0028066635131836
Step: 245	 Data: torch.Size([10, 384])	 Training Loss: 0.9572703242301941
Step: 246	 Data: torch.Size([10, 384])	 Training Loss: 1.0232011079788208
Step: 247	 Data: torch.Size([10, 384])	 Training Loss: 0.8972023725509644
Step: 248	 Data: torch.Size([10, 384])	 Training Loss: 0.8276869058609009
Step: 249	 Data: torch.Size([10, 384])	 Training Loss: 0.8984277844429016
Step: 250	 Data: torch.Size([10, 384])	 Training Loss: 1.0335811376571655
Step: 251	 Data: torch.Size([10, 384])	 Training Loss: 0.9987725019454956
Step: 252	 Data: torch.Size([10, 384])	 Training Loss: 0.994799792766571
Step: 253	 Data: torch.Size([10, 384])	 Training Loss: 0.972501277923584
Step: 254	 Data: torch.Size([10, 384])	 Training Loss: 0.9303942918777466
Step: 255	 Data: torch.Size([10, 384])	 Training Loss: 0.854751467704773
Step: 256	 Data: torch.Size([10, 384])	 Training Loss: 0.9451701045036316
Step: 257	 Data: torch.Size([10, 384])	 Training Loss: 0.8984572887420654
Step: 258	 Data: torch.Size([10, 384])	 Training Loss: 0.8982357382774353
Step: 259	 Data: torch.Size([10, 384])	 Training Loss: 0.8400213718414307
Step: 260	 Data: torch.Size([10, 384])	 Training Loss: 0.8955181837081909
Step: 261	 Data: torch.Size([10, 384])	 Training Loss: 1.0292034149169922
Step: 262	 Data: torch.Size([10, 384])	 Training Loss: 0.974856972694397
Step: 263	 Data: torch.Size([10, 384])	 Training Loss: 1.0492334365844727
Step: 264	 Data: torch.Size([10, 384])	 Training Loss: 1.0515434741973877
Step: 265	 Data: torch.Size([10, 384])	 Training Loss: 0.9422162771224976
Step: 266	 Data: torch.Size([10, 384])	 Training Loss: 0.8847944140434265
Step: 267	 Data: torch.Size([10, 384])	 Training Loss: 0.8799867630004883
Step: 268	 Data: torch.Size([10, 384])	 Training Loss: 0.900380551815033
Step: 269	 Data: torch.Size([10, 384])	 Training Loss: 0.8838279843330383
Step: 270	 Data: torch.Size([10, 384])	 Training Loss: 0.9883918762207031
Step: 271	 Data: torch.Size([10, 384])	 Training Loss: 0.953312337398529
Step: 272	 Data: torch.Size([10, 384])	 Training Loss: 0.9069711565971375
Step: 273	 Data: torch.Size([10, 384])	 Training Loss: 0.8207334280014038
Step: 274	 Data: torch.Size([10, 384])	 Training Loss: 0.9710250496864319
Step: 275	 Data: torch.Size([10, 384])	 Training Loss: 1.0075535774230957
Step: 276	 Data: torch.Size([10, 384])	 Training Loss: 0.9404987096786499
Step: 277	 Data: torch.Size([10, 384])	 Training Loss: 0.9099774360656738
Step: 278	 Data: torch.Size([10, 384])	 Training Loss: 1.0225616693496704
Step: 279	 Data: torch.Size([10, 384])	 Training Loss: 0.9733746647834778
Step: 280	 Data: torch.Size([10, 384])	 Training Loss: 0.9650551676750183
Step: 281	 Data: torch.Size([10, 384])	 Training Loss: 0.9942333102226257
Step: 282	 Data: torch.Size([10, 384])	 Training Loss: 0.9299777150154114
Step: 283	 Data: torch.Size([10, 384])	 Training Loss: 1.0497679710388184
Step: 284	 Data: torch.Size([10, 384])	 Training Loss: 1.2725200653076172
Step: 285	 Data: torch.Size([10, 384])	 Training Loss: 0.9259170293807983
Step: 286	 Data: torch.Size([10, 384])	 Training Loss: 1.0592141151428223
Step: 287	 Data: torch.Size([10, 384])	 Training Loss: 0.9083006381988525
Step: 288	 Data: torch.Size([10, 384])	 Training Loss: 0.7756690382957458
Step: 289	 Data: torch.Size([10, 384])	 Training Loss: 0.9314823150634766
Step: 290	 Data: torch.Size([10, 384])	 Training Loss: 0.9165711402893066
Step: 291	 Data: torch.Size([10, 384])	 Training Loss: 0.9475175738334656
Step: 292	 Data: torch.Size([10, 384])	 Training Loss: 1.0282648801803589
Step: 293	 Data: torch.Size([10, 384])	 Training Loss: 0.8550975918769836
Step: 294	 Data: torch.Size([10, 384])	 Training Loss: 0.8541877269744873
Step: 295	 Data: torch.Size([10, 384])	 Training Loss: 0.9844595193862915
Step: 296	 Data: torch.Size([10, 384])	 Training Loss: 1.0074360370635986
Step: 297	 Data: torch.Size([10, 384])	 Training Loss: 0.9398612976074219
Step: 298	 Data: torch.Size([10, 384])	 Training Loss: 0.9398311376571655
Step: 299	 Data: torch.Size([10, 384])	 Training Loss: 1.0078538656234741
Step: 300	 Data: torch.Size([10, 384])	 Training Loss: 1.0459718704223633
