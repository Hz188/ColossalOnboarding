[2024-05-08 02:11:27,378] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-05-08 02:11:27,378] torch.distributed.run: [WARNING] 
[2024-05-08 02:11:27,378] torch.distributed.run: [WARNING] *****************************************
[2024-05-08 02:11:27,378] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-08 02:11:27,378] torch.distributed.run: [WARNING] *****************************************
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
[W socket.cpp:663] [c10d] The client socket has failed to connect to [H100-201]:58081 (errno: 22 - Invalid argument).
====================initialize the distributed env====================
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.06s/it]
====================Use tensor parallel====================
====================convert origin mlp to parallel mlp====================
====================get the model with lora====================
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.59s/it]
trainable params: 4,194,304 || all params: 4,578,349,056 || trainable%: 0.09161171305851609
====================Use gradient checkpoint====================
====================Use distributed data parallel====================
====================get the dataset====================
====================get the dataloader====================
====================start train====================
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Step: 1	 Data: torch.Size([10, 384])	 Training Loss: 1.0071995258331299
Step: 2	 Data: torch.Size([10, 384])	 Training Loss: 1.0455574989318848
Step: 3	 Data: torch.Size([10, 384])	 Training Loss: 1.1252485513687134
Step: 4	 Data: torch.Size([10, 384])	 Training Loss: 1.07794189453125
Step: 5	 Data: torch.Size([10, 384])	 Training Loss: 1.0076186656951904
Step: 6	 Data: torch.Size([10, 384])	 Training Loss: 1.144310712814331
Step: 7	 Data: torch.Size([10, 384])	 Training Loss: 1.027605414390564
Step: 8	 Data: torch.Size([10, 384])	 Training Loss: 1.2210332155227661
Step: 9	 Data: torch.Size([10, 384])	 Training Loss: 1.082639455795288
Step: 10	 Data: torch.Size([10, 384])	 Training Loss: 1.0869184732437134
Step: 11	 Data: torch.Size([10, 384])	 Training Loss: 1.0137463808059692
Step: 12	 Data: torch.Size([10, 384])	 Training Loss: 1.0505805015563965
Step: 13	 Data: torch.Size([10, 384])	 Training Loss: 1.0892497301101685
Step: 14	 Data: torch.Size([10, 384])	 Training Loss: 1.2322977781295776
Step: 15	 Data: torch.Size([10, 384])	 Training Loss: 1.037783145904541
Step: 16	 Data: torch.Size([10, 384])	 Training Loss: 1.1248652935028076
Step: 17	 Data: torch.Size([10, 384])	 Training Loss: 1.1550012826919556
Step: 18	 Data: torch.Size([10, 384])	 Training Loss: 1.0539846420288086
Step: 19	 Data: torch.Size([10, 384])	 Training Loss: 1.213728666305542
Step: 20	 Data: torch.Size([10, 384])	 Training Loss: 1.0161443948745728
Step: 21	 Data: torch.Size([10, 384])	 Training Loss: 1.0230904817581177
Step: 22	 Data: torch.Size([10, 384])	 Training Loss: 1.0418182611465454
Step: 23	 Data: torch.Size([10, 384])	 Training Loss: 1.1345518827438354
Step: 24	 Data: torch.Size([10, 384])	 Training Loss: 1.1645679473876953
Step: 25	 Data: torch.Size([10, 384])	 Training Loss: 0.9766668081283569
Step: 26	 Data: torch.Size([10, 384])	 Training Loss: 0.9525215029716492
Step: 27	 Data: torch.Size([10, 384])	 Training Loss: 1.2122071981430054
Step: 28	 Data: torch.Size([10, 384])	 Training Loss: 1.1054372787475586
Step: 29	 Data: torch.Size([10, 384])	 Training Loss: 1.1103217601776123
Step: 30	 Data: torch.Size([10, 384])	 Training Loss: 0.9717496633529663
Step: 31	 Data: torch.Size([10, 384])	 Training Loss: 1.1124584674835205
Step: 32	 Data: torch.Size([10, 384])	 Training Loss: 1.1109479665756226
Step: 33	 Data: torch.Size([10, 384])	 Training Loss: 1.0167187452316284
Step: 34	 Data: torch.Size([10, 384])	 Training Loss: 0.8908133506774902
Step: 35	 Data: torch.Size([10, 384])	 Training Loss: 1.021903395652771
Step: 36	 Data: torch.Size([10, 384])	 Training Loss: 1.049109935760498
Step: 37	 Data: torch.Size([10, 384])	 Training Loss: 1.0969268083572388
Step: 38	 Data: torch.Size([10, 384])	 Training Loss: 0.948896050453186
Step: 39	 Data: torch.Size([10, 384])	 Training Loss: 0.9954838156700134
Step: 40	 Data: torch.Size([10, 384])	 Training Loss: 1.0222817659378052
Step: 41	 Data: torch.Size([10, 384])	 Training Loss: 1.154660940170288
Step: 42	 Data: torch.Size([10, 384])	 Training Loss: 0.9963927865028381
Step: 43	 Data: torch.Size([10, 384])	 Training Loss: 1.08175790309906
Step: 44	 Data: torch.Size([10, 384])	 Training Loss: 1.2318930625915527
Step: 45	 Data: torch.Size([10, 384])	 Training Loss: 1.0828479528427124
Step: 46	 Data: torch.Size([10, 384])	 Training Loss: 0.9616149067878723
Step: 47	 Data: torch.Size([10, 384])	 Training Loss: 0.9655551314353943
Step: 48	 Data: torch.Size([10, 384])	 Training Loss: 1.1907389163970947
Step: 49	 Data: torch.Size([10, 384])	 Training Loss: 1.1353574991226196
Step: 50	 Data: torch.Size([10, 384])	 Training Loss: 1.1873024702072144
Step: 51	 Data: torch.Size([10, 384])	 Training Loss: 1.000243902206421
Step: 52	 Data: torch.Size([10, 384])	 Training Loss: 1.0058261156082153
Step: 53	 Data: torch.Size([10, 384])	 Training Loss: 1.0807679891586304
Step: 54	 Data: torch.Size([10, 384])	 Training Loss: 1.0625489950180054
Step: 55	 Data: torch.Size([10, 384])	 Training Loss: 0.9652861952781677
Step: 56	 Data: torch.Size([10, 384])	 Training Loss: 1.1044843196868896
Step: 57	 Data: torch.Size([10, 384])	 Training Loss: 0.9607232809066772
Step: 58	 Data: torch.Size([10, 384])	 Training Loss: 0.8900121450424194
Step: 59	 Data: torch.Size([10, 384])	 Training Loss: 1.1169627904891968
Step: 60	 Data: torch.Size([10, 384])	 Training Loss: 1.1076184511184692
Step: 61	 Data: torch.Size([10, 384])	 Training Loss: 0.9423165917396545
Step: 62	 Data: torch.Size([10, 384])	 Training Loss: 1.1786119937896729
Step: 63	 Data: torch.Size([10, 384])	 Training Loss: 1.0599281787872314
Step: 64	 Data: torch.Size([10, 384])	 Training Loss: 0.9940611720085144
Step: 65	 Data: torch.Size([10, 384])	 Training Loss: 0.9843655228614807
Step: 66	 Data: torch.Size([10, 384])	 Training Loss: 1.0101467370986938
Step: 67	 Data: torch.Size([10, 384])	 Training Loss: 1.014193058013916
Step: 68	 Data: torch.Size([10, 384])	 Training Loss: 1.1471738815307617
Step: 69	 Data: torch.Size([10, 384])	 Training Loss: 1.1730562448501587
Step: 70	 Data: torch.Size([10, 384])	 Training Loss: 0.9294843673706055
Step: 71	 Data: torch.Size([10, 384])	 Training Loss: 0.9460309147834778
Step: 72	 Data: torch.Size([10, 350])	 Training Loss: 1.429434895515442
Step: 73	 Data: torch.Size([10, 384])	 Training Loss: 0.9302147626876831
Step: 74	 Data: torch.Size([10, 384])	 Training Loss: 1.0670713186264038
Step: 75	 Data: torch.Size([10, 384])	 Training Loss: 0.9793694615364075
Step: 76	 Data: torch.Size([10, 384])	 Training Loss: 0.8634474873542786
Step: 77	 Data: torch.Size([10, 384])	 Training Loss: 1.1126751899719238
Step: 78	 Data: torch.Size([10, 384])	 Training Loss: 1.0245646238327026
Step: 79	 Data: torch.Size([10, 384])	 Training Loss: 1.2335481643676758
Step: 80	 Data: torch.Size([10, 384])	 Training Loss: 1.0619337558746338
Step: 81	 Data: torch.Size([10, 384])	 Training Loss: 1.033090591430664
Step: 82	 Data: torch.Size([10, 384])	 Training Loss: 1.0644043684005737
Step: 83	 Data: torch.Size([10, 384])	 Training Loss: 1.0409319400787354
Step: 84	 Data: torch.Size([10, 384])	 Training Loss: 0.8629711866378784
Step: 85	 Data: torch.Size([10, 384])	 Training Loss: 0.9676556587219238
Step: 86	 Data: torch.Size([10, 384])	 Training Loss: 1.1031010150909424
Step: 87	 Data: torch.Size([10, 384])	 Training Loss: 1.0243724584579468
Step: 88	 Data: torch.Size([10, 384])	 Training Loss: 0.8805351257324219
Step: 89	 Data: torch.Size([10, 384])	 Training Loss: 1.0766392946243286
Step: 90	 Data: torch.Size([10, 384])	 Training Loss: 1.0484650135040283
Step: 91	 Data: torch.Size([10, 384])	 Training Loss: 0.9021714925765991
Step: 92	 Data: torch.Size([10, 384])	 Training Loss: 0.8847752213478088
Step: 93	 Data: torch.Size([10, 384])	 Training Loss: 0.9990934133529663
Step: 94	 Data: torch.Size([10, 384])	 Training Loss: 1.0969396829605103
Step: 95	 Data: torch.Size([10, 384])	 Training Loss: 0.9577993154525757
Step: 96	 Data: torch.Size([10, 384])	 Training Loss: 0.8950464129447937
Step: 97	 Data: torch.Size([10, 384])	 Training Loss: 1.0507144927978516
Step: 98	 Data: torch.Size([10, 384])	 Training Loss: 0.9112752079963684
Step: 99	 Data: torch.Size([10, 384])	 Training Loss: 0.9802984595298767
Step: 100	 Data: torch.Size([10, 384])	 Training Loss: 1.1402392387390137
Step: 101	 Data: torch.Size([10, 384])	 Training Loss: 0.9613850712776184
Step: 102	 Data: torch.Size([10, 384])	 Training Loss: 0.9705130457878113
Step: 103	 Data: torch.Size([10, 384])	 Training Loss: 0.9980162978172302
Step: 104	 Data: torch.Size([10, 384])	 Training Loss: 0.9384273886680603
Step: 105	 Data: torch.Size([10, 384])	 Training Loss: 0.9841593503952026
Step: 106	 Data: torch.Size([10, 384])	 Training Loss: 1.1119942665100098
Step: 107	 Data: torch.Size([10, 384])	 Training Loss: 0.9330657720565796
Step: 108	 Data: torch.Size([10, 384])	 Training Loss: 0.9074214100837708
Step: 109	 Data: torch.Size([10, 384])	 Training Loss: 1.0000239610671997
Step: 110	 Data: torch.Size([10, 384])	 Training Loss: 0.9130789041519165
Step: 111	 Data: torch.Size([10, 384])	 Training Loss: 1.1976298093795776
Step: 112	 Data: torch.Size([10, 384])	 Training Loss: 1.1936317682266235
Step: 113	 Data: torch.Size([10, 384])	 Training Loss: 1.0611162185668945
Step: 114	 Data: torch.Size([10, 384])	 Training Loss: 0.9722595810890198
Step: 115	 Data: torch.Size([10, 384])	 Training Loss: 0.8919511437416077
Step: 116	 Data: torch.Size([10, 384])	 Training Loss: 0.9154934287071228
Step: 117	 Data: torch.Size([10, 384])	 Training Loss: 0.894728422164917
Step: 118	 Data: torch.Size([10, 384])	 Training Loss: 0.9370478391647339
Step: 119	 Data: torch.Size([10, 384])	 Training Loss: 1.3181860446929932
Step: 120	 Data: torch.Size([10, 384])	 Training Loss: 0.8715943694114685
Step: 121	 Data: torch.Size([10, 307])	 Training Loss: 1.0796213150024414
Step: 122	 Data: torch.Size([10, 384])	 Training Loss: 1.1305766105651855
Step: 123	 Data: torch.Size([10, 384])	 Training Loss: 0.9751193523406982
Step: 124	 Data: torch.Size([10, 384])	 Training Loss: 0.8693236112594604
Step: 125	 Data: torch.Size([10, 384])	 Training Loss: 0.8865723609924316
Step: 126	 Data: torch.Size([10, 384])	 Training Loss: 0.9123422503471375
Step: 127	 Data: torch.Size([10, 384])	 Training Loss: 1.03910493850708
Step: 128	 Data: torch.Size([10, 384])	 Training Loss: 0.8361175656318665
Step: 129	 Data: torch.Size([10, 384])	 Training Loss: 1.081292986869812
Step: 130	 Data: torch.Size([10, 384])	 Training Loss: 0.9684640169143677
Step: 131	 Data: torch.Size([10, 384])	 Training Loss: 0.9668298363685608
Step: 132	 Data: torch.Size([10, 384])	 Training Loss: 0.9546060562133789
Step: 133	 Data: torch.Size([10, 384])	 Training Loss: 0.995914876461029
Step: 134	 Data: torch.Size([10, 384])	 Training Loss: 1.0623013973236084
Step: 135	 Data: torch.Size([10, 384])	 Training Loss: 0.9766867160797119
Step: 136	 Data: torch.Size([10, 384])	 Training Loss: 0.9295387864112854
Step: 137	 Data: torch.Size([10, 384])	 Training Loss: 1.0558207035064697
Step: 138	 Data: torch.Size([10, 384])	 Training Loss: 0.9159905314445496
Step: 139	 Data: torch.Size([10, 384])	 Training Loss: 1.0017229318618774
Step: 140	 Data: torch.Size([10, 384])	 Training Loss: 0.9583576321601868
Step: 141	 Data: torch.Size([10, 384])	 Training Loss: 0.9861751198768616
Step: 142	 Data: torch.Size([10, 384])	 Training Loss: 0.9699482917785645
Step: 143	 Data: torch.Size([10, 384])	 Training Loss: 1.0090349912643433
Step: 144	 Data: torch.Size([10, 384])	 Training Loss: 1.0093337297439575
Step: 145	 Data: torch.Size([10, 384])	 Training Loss: 0.9483555555343628
Step: 146	 Data: torch.Size([10, 384])	 Training Loss: 1.0053772926330566
Step: 147	 Data: torch.Size([10, 384])	 Training Loss: 0.9847990870475769
Step: 148	 Data: torch.Size([10, 384])	 Training Loss: 0.9338104724884033
Step: 149	 Data: torch.Size([10, 384])	 Training Loss: 0.8998034596443176
Step: 150	 Data: torch.Size([10, 384])	 Training Loss: 0.968419075012207
Step: 151	 Data: torch.Size([10, 384])	 Training Loss: 1.054809808731079
Step: 152	 Data: torch.Size([10, 384])	 Training Loss: 0.8956639170646667
Step: 153	 Data: torch.Size([10, 384])	 Training Loss: 1.110269546508789
Step: 154	 Data: torch.Size([10, 384])	 Training Loss: 1.0246154069900513
Step: 155	 Data: torch.Size([10, 384])	 Training Loss: 0.9324905872344971
Step: 156	 Data: torch.Size([10, 384])	 Training Loss: 0.8404693007469177
Step: 157	 Data: torch.Size([10, 384])	 Training Loss: 0.8609216809272766
Step: 158	 Data: torch.Size([10, 384])	 Training Loss: 0.9796181917190552
Step: 159	 Data: torch.Size([10, 384])	 Training Loss: 1.0539597272872925
Step: 160	 Data: torch.Size([10, 384])	 Training Loss: 0.8685765862464905
Step: 161	 Data: torch.Size([10, 384])	 Training Loss: 1.0340861082077026
Step: 162	 Data: torch.Size([10, 384])	 Training Loss: 1.0011634826660156
Step: 163	 Data: torch.Size([10, 384])	 Training Loss: 0.9838399291038513
Step: 164	 Data: torch.Size([10, 384])	 Training Loss: 0.9503547549247742
Step: 165	 Data: torch.Size([10, 384])	 Training Loss: 0.9059758186340332
Step: 166	 Data: torch.Size([10, 384])	 Training Loss: 1.0804718732833862
Step: 167	 Data: torch.Size([10, 384])	 Training Loss: 0.9242451786994934
Step: 168	 Data: torch.Size([10, 384])	 Training Loss: 1.0214282274246216
Step: 169	 Data: torch.Size([10, 384])	 Training Loss: 0.9644215703010559
Step: 170	 Data: torch.Size([10, 384])	 Training Loss: 1.0725898742675781
Step: 171	 Data: torch.Size([10, 384])	 Training Loss: 0.9119431972503662
Step: 172	 Data: torch.Size([10, 384])	 Training Loss: 0.8482328057289124
Step: 173	 Data: torch.Size([10, 384])	 Training Loss: 0.9172387719154358
Step: 174	 Data: torch.Size([10, 384])	 Training Loss: 0.9028223156929016
Step: 175	 Data: torch.Size([10, 384])	 Training Loss: 1.0383378267288208
Step: 176	 Data: torch.Size([10, 384])	 Training Loss: 0.9516938924789429
Step: 177	 Data: torch.Size([10, 384])	 Training Loss: 0.9921603202819824
Step: 178	 Data: torch.Size([10, 384])	 Training Loss: 0.9008608460426331
Step: 179	 Data: torch.Size([10, 384])	 Training Loss: 0.8353881239891052
Step: 180	 Data: torch.Size([10, 384])	 Training Loss: 1.0486077070236206
Step: 181	 Data: torch.Size([10, 384])	 Training Loss: 0.7367228865623474
Step: 182	 Data: torch.Size([10, 384])	 Training Loss: 1.081033706665039
Step: 183	 Data: torch.Size([10, 384])	 Training Loss: 0.8644119501113892
Step: 184	 Data: torch.Size([10, 384])	 Training Loss: 0.9479132294654846
Step: 185	 Data: torch.Size([10, 384])	 Training Loss: 0.8513462543487549
Step: 186	 Data: torch.Size([10, 384])	 Training Loss: 0.9869133830070496
Step: 187	 Data: torch.Size([10, 384])	 Training Loss: 0.8687936663627625
Step: 188	 Data: torch.Size([10, 384])	 Training Loss: 0.9687638878822327
Step: 189	 Data: torch.Size([10, 384])	 Training Loss: 0.9288225769996643
Step: 190	 Data: torch.Size([10, 384])	 Training Loss: 1.0982517004013062
Step: 191	 Data: torch.Size([10, 384])	 Training Loss: 0.8909883499145508
Step: 192	 Data: torch.Size([10, 384])	 Training Loss: 1.006801724433899
Step: 193	 Data: torch.Size([10, 384])	 Training Loss: 0.9368116855621338
Step: 194	 Data: torch.Size([10, 384])	 Training Loss: 0.8806284666061401
Step: 195	 Data: torch.Size([10, 384])	 Training Loss: 0.9391549825668335
Step: 196	 Data: torch.Size([10, 384])	 Training Loss: 0.9155484437942505
Step: 197	 Data: torch.Size([10, 384])	 Training Loss: 0.83961421251297
Step: 198	 Data: torch.Size([10, 384])	 Training Loss: 1.016646385192871
Step: 199	 Data: torch.Size([10, 384])	 Training Loss: 1.0066099166870117
Step: 200	 Data: torch.Size([10, 384])	 Training Loss: 0.9046317338943481
Step: 201	 Data: torch.Size([10, 384])	 Training Loss: 0.9708083868026733
Step: 202	 Data: torch.Size([10, 384])	 Training Loss: 0.8999365568161011
Step: 203	 Data: torch.Size([10, 384])	 Training Loss: 1.0076223611831665
Step: 204	 Data: torch.Size([10, 384])	 Training Loss: 0.9767236113548279
Step: 205	 Data: torch.Size([10, 384])	 Training Loss: 0.9945946931838989
Step: 206	 Data: torch.Size([10, 384])	 Training Loss: 0.8994596004486084
Step: 207	 Data: torch.Size([10, 384])	 Training Loss: 0.8734729290008545
Step: 208	 Data: torch.Size([10, 384])	 Training Loss: 0.9838303923606873
Step: 209	 Data: torch.Size([10, 384])	 Training Loss: 0.9522044062614441
Step: 210	 Data: torch.Size([10, 384])	 Training Loss: 0.9875696897506714
Step: 211	 Data: torch.Size([10, 384])	 Training Loss: 0.8946393132209778
Step: 212	 Data: torch.Size([10, 384])	 Training Loss: 0.8841864466667175
Step: 213	 Data: torch.Size([10, 384])	 Training Loss: 0.9293519854545593
Step: 214	 Data: torch.Size([10, 384])	 Training Loss: 1.0805950164794922
Step: 215	 Data: torch.Size([10, 384])	 Training Loss: 0.9054872393608093
Step: 216	 Data: torch.Size([10, 384])	 Training Loss: 0.915476381778717
Step: 217	 Data: torch.Size([10, 384])	 Training Loss: 1.037194013595581
Step: 218	 Data: torch.Size([10, 384])	 Training Loss: 0.9836693406105042
Step: 219	 Data: torch.Size([10, 384])	 Training Loss: 1.045361042022705
Step: 220	 Data: torch.Size([10, 384])	 Training Loss: 0.8756075501441956
Step: 221	 Data: torch.Size([10, 384])	 Training Loss: 1.009931206703186
Step: 222	 Data: torch.Size([10, 384])	 Training Loss: 0.8536397218704224
Step: 223	 Data: torch.Size([10, 384])	 Training Loss: 0.9302148222923279
Step: 224	 Data: torch.Size([10, 384])	 Training Loss: 0.931854248046875
Step: 225	 Data: torch.Size([10, 384])	 Training Loss: 0.8154716491699219
Step: 226	 Data: torch.Size([10, 384])	 Training Loss: 0.8586020469665527
Step: 227	 Data: torch.Size([10, 384])	 Training Loss: 0.9832515716552734
Step: 228	 Data: torch.Size([10, 384])	 Training Loss: 0.9067265391349792
Step: 229	 Data: torch.Size([10, 384])	 Training Loss: 0.8963531851768494
Step: 230	 Data: torch.Size([10, 384])	 Training Loss: 0.8236461281776428
Step: 231	 Data: torch.Size([10, 384])	 Training Loss: 0.9445934295654297
Step: 232	 Data: torch.Size([10, 384])	 Training Loss: 0.9847553372383118
Step: 233	 Data: torch.Size([10, 384])	 Training Loss: 0.9726777076721191
Step: 234	 Data: torch.Size([10, 384])	 Training Loss: 0.8217533230781555
Step: 235	 Data: torch.Size([10, 384])	 Training Loss: 0.8217459917068481
Step: 236	 Data: torch.Size([10, 384])	 Training Loss: 0.9465133547782898
Step: 237	 Data: torch.Size([10, 384])	 Training Loss: 1.022516131401062
Step: 238	 Data: torch.Size([10, 384])	 Training Loss: 1.0356847047805786
Step: 239	 Data: torch.Size([10, 384])	 Training Loss: 0.9325883388519287
Step: 240	 Data: torch.Size([10, 384])	 Training Loss: 0.9462347626686096
Step: 241	 Data: torch.Size([10, 384])	 Training Loss: 1.0212090015411377
Step: 242	 Data: torch.Size([10, 384])	 Training Loss: 0.9069845080375671
Step: 243	 Data: torch.Size([10, 384])	 Training Loss: 1.0606164932250977
Step: 244	 Data: torch.Size([10, 384])	 Training Loss: 1.0027750730514526
Step: 245	 Data: torch.Size([10, 384])	 Training Loss: 0.9568669199943542
Step: 246	 Data: torch.Size([10, 384])	 Training Loss: 1.023123025894165
Step: 247	 Data: torch.Size([10, 384])	 Training Loss: 0.8973237872123718
Step: 248	 Data: torch.Size([10, 384])	 Training Loss: 0.827714741230011
Step: 249	 Data: torch.Size([10, 384])	 Training Loss: 0.8988686800003052
Step: 250	 Data: torch.Size([10, 384])	 Training Loss: 1.0344918966293335
Step: 251	 Data: torch.Size([10, 384])	 Training Loss: 0.998536229133606
Step: 252	 Data: torch.Size([10, 384])	 Training Loss: 0.9948237538337708
Step: 253	 Data: torch.Size([10, 384])	 Training Loss: 0.9714739322662354
Step: 254	 Data: torch.Size([10, 384])	 Training Loss: 0.9312940835952759
Step: 255	 Data: torch.Size([10, 384])	 Training Loss: 0.855421245098114
Step: 256	 Data: torch.Size([10, 384])	 Training Loss: 0.9462347626686096
Step: 257	 Data: torch.Size([10, 384])	 Training Loss: 0.8979296684265137
Step: 258	 Data: torch.Size([10, 384])	 Training Loss: 0.8988271951675415
Step: 259	 Data: torch.Size([10, 384])	 Training Loss: 0.840356707572937
Step: 260	 Data: torch.Size([10, 384])	 Training Loss: 0.8964889645576477
Step: 261	 Data: torch.Size([10, 384])	 Training Loss: 1.028914213180542
Step: 262	 Data: torch.Size([10, 384])	 Training Loss: 0.9758161902427673
Step: 263	 Data: torch.Size([10, 384])	 Training Loss: 1.049526572227478
Step: 264	 Data: torch.Size([10, 384])	 Training Loss: 1.0507540702819824
Step: 265	 Data: torch.Size([10, 384])	 Training Loss: 0.9428457617759705
Step: 266	 Data: torch.Size([10, 384])	 Training Loss: 0.8844621181488037
Step: 267	 Data: torch.Size([10, 384])	 Training Loss: 0.8800126314163208
Step: 268	 Data: torch.Size([10, 384])	 Training Loss: 0.9010598659515381
Step: 269	 Data: torch.Size([10, 384])	 Training Loss: 0.883724570274353
Step: 270	 Data: torch.Size([10, 384])	 Training Loss: 0.9884898662567139
Step: 271	 Data: torch.Size([10, 384])	 Training Loss: 0.9532796740531921
Step: 272	 Data: torch.Size([10, 384])	 Training Loss: 0.9070143103599548
Step: 273	 Data: torch.Size([10, 384])	 Training Loss: 0.8208785057067871
Step: 274	 Data: torch.Size([10, 384])	 Training Loss: 0.9707322716712952
Step: 275	 Data: torch.Size([10, 384])	 Training Loss: 1.0080455541610718
Step: 276	 Data: torch.Size([10, 384])	 Training Loss: 0.9414584040641785
Step: 277	 Data: torch.Size([10, 384])	 Training Loss: 0.910017192363739
Step: 278	 Data: torch.Size([10, 384])	 Training Loss: 1.0226060152053833
Step: 279	 Data: torch.Size([10, 384])	 Training Loss: 0.9732127785682678
Step: 280	 Data: torch.Size([10, 384])	 Training Loss: 0.9651485085487366
Step: 281	 Data: torch.Size([10, 384])	 Training Loss: 0.9947277903556824
Step: 282	 Data: torch.Size([10, 384])	 Training Loss: 0.930313229560852
Step: 283	 Data: torch.Size([10, 384])	 Training Loss: 1.05036461353302
Step: 284	 Data: torch.Size([10, 384])	 Training Loss: 1.2725605964660645
Step: 285	 Data: torch.Size([10, 384])	 Training Loss: 0.9265695810317993
Step: 286	 Data: torch.Size([10, 384])	 Training Loss: 1.0591727495193481
Step: 287	 Data: torch.Size([10, 384])	 Training Loss: 0.9094530344009399
Step: 288	 Data: torch.Size([10, 384])	 Training Loss: 0.774986743927002
Step: 289	 Data: torch.Size([10, 384])	 Training Loss: 0.9315866827964783
Step: 290	 Data: torch.Size([10, 384])	 Training Loss: 0.9159752726554871
Step: 291	 Data: torch.Size([10, 384])	 Training Loss: 0.9480970501899719
Step: 292	 Data: torch.Size([10, 384])	 Training Loss: 1.0280519723892212
Step: 293	 Data: torch.Size([10, 384])	 Training Loss: 0.8556609153747559
Step: 294	 Data: torch.Size([10, 384])	 Training Loss: 0.8536690473556519
Step: 295	 Data: torch.Size([10, 384])	 Training Loss: 0.9864757657051086
Step: 296	 Data: torch.Size([10, 384])	 Training Loss: 1.0074831247329712
Step: 297	 Data: torch.Size([10, 384])	 Training Loss: 0.9406948089599609
Step: 298	 Data: torch.Size([10, 384])	 Training Loss: 0.9393030405044556
Step: 299	 Data: torch.Size([10, 384])	 Training Loss: 1.007042646408081
Step: 300	 Data: torch.Size([10, 384])	 Training Loss: 1.0462397336959839
