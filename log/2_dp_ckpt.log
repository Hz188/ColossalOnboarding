[2024-05-08 02:21:51,940] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-05-08 02:21:51,940] torch.distributed.run: [WARNING] 
[2024-05-08 02:21:51,940] torch.distributed.run: [WARNING] *****************************************
[2024-05-08 02:21:51,940] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-08 02:21:51,940] torch.distributed.run: [WARNING] *****************************************
====================initialize the distributed env====================
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.17s/it]
====================get the model with lora====================
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.16s/it]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
====================Use gradient checkpoint====================
====================Use distributed data parallel====================
====================get the dataset====================
====================get the dataloader====================
====================start train====================
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Step: 1	 Data: torch.Size([10, 384])	 Training Loss: 1.0071995258331299
Step: 2	 Data: torch.Size([10, 384])	 Training Loss: 1.0455563068389893
Step: 3	 Data: torch.Size([10, 384])	 Training Loss: 1.12528395652771
Step: 4	 Data: torch.Size([10, 384])	 Training Loss: 1.077960729598999
Step: 5	 Data: torch.Size([10, 384])	 Training Loss: 1.0077317953109741
Step: 6	 Data: torch.Size([10, 384])	 Training Loss: 1.144497036933899
Step: 7	 Data: torch.Size([10, 384])	 Training Loss: 1.027693748474121
Step: 8	 Data: torch.Size([10, 384])	 Training Loss: 1.221079707145691
Step: 9	 Data: torch.Size([10, 384])	 Training Loss: 1.08279287815094
Step: 10	 Data: torch.Size([10, 384])	 Training Loss: 1.087111473083496
Step: 11	 Data: torch.Size([10, 384])	 Training Loss: 1.0139505863189697
Step: 12	 Data: torch.Size([10, 384])	 Training Loss: 1.0507060289382935
Step: 13	 Data: torch.Size([10, 384])	 Training Loss: 1.0893858671188354
Step: 14	 Data: torch.Size([10, 384])	 Training Loss: 1.2325350046157837
Step: 15	 Data: torch.Size([10, 384])	 Training Loss: 1.0379582643508911
Step: 16	 Data: torch.Size([10, 384])	 Training Loss: 1.1252912282943726
Step: 17	 Data: torch.Size([10, 384])	 Training Loss: 1.1551158428192139
Step: 18	 Data: torch.Size([10, 384])	 Training Loss: 1.0542081594467163
Step: 19	 Data: torch.Size([10, 384])	 Training Loss: 1.2139973640441895
Step: 20	 Data: torch.Size([10, 384])	 Training Loss: 1.0165919065475464
Step: 21	 Data: torch.Size([10, 384])	 Training Loss: 1.023639440536499
Step: 22	 Data: torch.Size([10, 384])	 Training Loss: 1.042292833328247
Step: 23	 Data: torch.Size([10, 384])	 Training Loss: 1.1349536180496216
Step: 24	 Data: torch.Size([10, 384])	 Training Loss: 1.1651012897491455
Step: 25	 Data: torch.Size([10, 384])	 Training Loss: 0.9769537448883057
Step: 26	 Data: torch.Size([10, 384])	 Training Loss: 0.9529433250427246
Step: 27	 Data: torch.Size([10, 384])	 Training Loss: 1.2126578092575073
Step: 28	 Data: torch.Size([10, 384])	 Training Loss: 1.1059490442276
Step: 29	 Data: torch.Size([10, 384])	 Training Loss: 1.110910177230835
Step: 30	 Data: torch.Size([10, 384])	 Training Loss: 0.9721893668174744
Step: 31	 Data: torch.Size([10, 384])	 Training Loss: 1.1131842136383057
Step: 32	 Data: torch.Size([10, 384])	 Training Loss: 1.111722707748413
Step: 33	 Data: torch.Size([10, 384])	 Training Loss: 1.0172370672225952
Step: 34	 Data: torch.Size([10, 384])	 Training Loss: 0.8912702202796936
Step: 35	 Data: torch.Size([10, 384])	 Training Loss: 1.0224924087524414
Step: 36	 Data: torch.Size([10, 384])	 Training Loss: 1.049627661705017
Step: 37	 Data: torch.Size([10, 384])	 Training Loss: 1.097509741783142
Step: 38	 Data: torch.Size([10, 384])	 Training Loss: 0.9496304392814636
Step: 39	 Data: torch.Size([10, 384])	 Training Loss: 0.9964064955711365
Step: 40	 Data: torch.Size([10, 384])	 Training Loss: 1.0228322744369507
Step: 41	 Data: torch.Size([10, 384])	 Training Loss: 1.1557387113571167
Step: 42	 Data: torch.Size([10, 384])	 Training Loss: 0.9969431757926941
Step: 43	 Data: torch.Size([10, 384])	 Training Loss: 1.0822365283966064
Step: 44	 Data: torch.Size([10, 384])	 Training Loss: 1.2321586608886719
Step: 45	 Data: torch.Size([10, 384])	 Training Loss: 1.0829980373382568
Step: 46	 Data: torch.Size([10, 384])	 Training Loss: 0.9623205661773682
Step: 47	 Data: torch.Size([10, 384])	 Training Loss: 0.9661489129066467
Step: 48	 Data: torch.Size([10, 384])	 Training Loss: 1.1922225952148438
Step: 49	 Data: torch.Size([10, 384])	 Training Loss: 1.13582181930542
Step: 50	 Data: torch.Size([10, 384])	 Training Loss: 1.1879628896713257
Step: 51	 Data: torch.Size([10, 384])	 Training Loss: 1.0012115240097046
Step: 52	 Data: torch.Size([10, 384])	 Training Loss: 1.0061808824539185
Step: 53	 Data: torch.Size([10, 384])	 Training Loss: 1.081127643585205
Step: 54	 Data: torch.Size([10, 384])	 Training Loss: 1.0628091096878052
Step: 55	 Data: torch.Size([10, 384])	 Training Loss: 0.9663383960723877
Step: 56	 Data: torch.Size([10, 384])	 Training Loss: 1.1059516668319702
Step: 57	 Data: torch.Size([10, 384])	 Training Loss: 0.9611345529556274
Step: 58	 Data: torch.Size([10, 384])	 Training Loss: 0.8897607922554016
Step: 59	 Data: torch.Size([10, 384])	 Training Loss: 1.1172581911087036
Step: 60	 Data: torch.Size([10, 384])	 Training Loss: 1.1079798936843872
Step: 61	 Data: torch.Size([10, 384])	 Training Loss: 0.9431294798851013
Step: 62	 Data: torch.Size([10, 384])	 Training Loss: 1.1793278455734253
Step: 63	 Data: torch.Size([10, 384])	 Training Loss: 1.0599554777145386
Step: 64	 Data: torch.Size([10, 384])	 Training Loss: 0.9943259358406067
Step: 65	 Data: torch.Size([10, 384])	 Training Loss: 0.9847412109375
Step: 66	 Data: torch.Size([10, 384])	 Training Loss: 1.010048747062683
Step: 67	 Data: torch.Size([10, 384])	 Training Loss: 1.0137193202972412
Step: 68	 Data: torch.Size([10, 384])	 Training Loss: 1.147716760635376
Step: 69	 Data: torch.Size([10, 384])	 Training Loss: 1.1732263565063477
Step: 70	 Data: torch.Size([10, 384])	 Training Loss: 0.9299972653388977
Step: 71	 Data: torch.Size([10, 384])	 Training Loss: 0.9461272954940796
Step: 72	 Data: torch.Size([10, 350])	 Training Loss: 1.4285788536071777
Step: 73	 Data: torch.Size([10, 384])	 Training Loss: 0.9301820397377014
Step: 74	 Data: torch.Size([10, 384])	 Training Loss: 1.0659781694412231
Step: 75	 Data: torch.Size([10, 384])	 Training Loss: 0.9785085320472717
Step: 76	 Data: torch.Size([10, 384])	 Training Loss: 0.8634185194969177
Step: 77	 Data: torch.Size([10, 384])	 Training Loss: 1.1122627258300781
Step: 78	 Data: torch.Size([10, 384])	 Training Loss: 1.0243911743164062
Step: 79	 Data: torch.Size([10, 384])	 Training Loss: 1.2331132888793945
Step: 80	 Data: torch.Size([10, 384])	 Training Loss: 1.060999870300293
Step: 81	 Data: torch.Size([10, 384])	 Training Loss: 1.033241868019104
Step: 82	 Data: torch.Size([10, 384])	 Training Loss: 1.0638740062713623
Step: 83	 Data: torch.Size([10, 384])	 Training Loss: 1.0401127338409424
Step: 84	 Data: torch.Size([10, 384])	 Training Loss: 0.8620553016662598
Step: 85	 Data: torch.Size([10, 384])	 Training Loss: 0.9656922221183777
Step: 86	 Data: torch.Size([10, 384])	 Training Loss: 1.10185706615448
Step: 87	 Data: torch.Size([10, 384])	 Training Loss: 1.023197889328003
Step: 88	 Data: torch.Size([10, 384])	 Training Loss: 0.8800892233848572
Step: 89	 Data: torch.Size([10, 384])	 Training Loss: 1.0758250951766968
Step: 90	 Data: torch.Size([10, 384])	 Training Loss: 1.0473957061767578
Step: 91	 Data: torch.Size([10, 384])	 Training Loss: 0.9001113772392273
Step: 92	 Data: torch.Size([10, 384])	 Training Loss: 0.8835534453392029
Step: 93	 Data: torch.Size([10, 384])	 Training Loss: 0.9978083372116089
Step: 94	 Data: torch.Size([10, 384])	 Training Loss: 1.0966745615005493
Step: 95	 Data: torch.Size([10, 384])	 Training Loss: 0.9561458826065063
Step: 96	 Data: torch.Size([10, 384])	 Training Loss: 0.8936534523963928
Step: 97	 Data: torch.Size([10, 384])	 Training Loss: 1.049769639968872
Step: 98	 Data: torch.Size([10, 384])	 Training Loss: 0.9101929664611816
Step: 99	 Data: torch.Size([10, 384])	 Training Loss: 0.9791650176048279
Step: 100	 Data: torch.Size([10, 384])	 Training Loss: 1.139631986618042
Step: 101	 Data: torch.Size([10, 384])	 Training Loss: 0.960689902305603
Step: 102	 Data: torch.Size([10, 384])	 Training Loss: 0.9700526595115662
Step: 103	 Data: torch.Size([10, 384])	 Training Loss: 0.9966175556182861
Step: 104	 Data: torch.Size([10, 384])	 Training Loss: 0.9371230602264404
Step: 105	 Data: torch.Size([10, 384])	 Training Loss: 0.9837729930877686
Step: 106	 Data: torch.Size([10, 384])	 Training Loss: 1.1123558282852173
Step: 107	 Data: torch.Size([10, 384])	 Training Loss: 0.9328903555870056
Step: 108	 Data: torch.Size([10, 384])	 Training Loss: 0.9056697487831116
Step: 109	 Data: torch.Size([10, 384])	 Training Loss: 0.9992799758911133
Step: 110	 Data: torch.Size([10, 384])	 Training Loss: 0.9124214053153992
Step: 111	 Data: torch.Size([10, 384])	 Training Loss: 1.1972131729125977
Step: 112	 Data: torch.Size([10, 384])	 Training Loss: 1.192773461341858
Step: 113	 Data: torch.Size([10, 384])	 Training Loss: 1.0603922605514526
Step: 114	 Data: torch.Size([10, 384])	 Training Loss: 0.9718872308731079
Step: 115	 Data: torch.Size([10, 384])	 Training Loss: 0.8908768892288208
Step: 116	 Data: torch.Size([10, 384])	 Training Loss: 0.9138360023498535
Step: 117	 Data: torch.Size([10, 384])	 Training Loss: 0.8939751386642456
Step: 118	 Data: torch.Size([10, 384])	 Training Loss: 0.9356284737586975
Step: 119	 Data: torch.Size([10, 384])	 Training Loss: 1.3172647953033447
Step: 120	 Data: torch.Size([10, 384])	 Training Loss: 0.8702787160873413
Step: 121	 Data: torch.Size([10, 307])	 Training Loss: 1.0765596628189087
Step: 122	 Data: torch.Size([10, 384])	 Training Loss: 1.1285258531570435
Step: 123	 Data: torch.Size([10, 384])	 Training Loss: 0.9738227128982544
Step: 124	 Data: torch.Size([10, 384])	 Training Loss: 0.8650259971618652
Step: 125	 Data: torch.Size([10, 384])	 Training Loss: 0.8847754597663879
Step: 126	 Data: torch.Size([10, 384])	 Training Loss: 0.9102761149406433
Step: 127	 Data: torch.Size([10, 384])	 Training Loss: 1.0376557111740112
Step: 128	 Data: torch.Size([10, 384])	 Training Loss: 0.8332737684249878
Step: 129	 Data: torch.Size([10, 384])	 Training Loss: 1.0781804323196411
Step: 130	 Data: torch.Size([10, 384])	 Training Loss: 0.9656118750572205
Step: 131	 Data: torch.Size([10, 384])	 Training Loss: 0.9646502733230591
Step: 132	 Data: torch.Size([10, 384])	 Training Loss: 0.9536609649658203
Step: 133	 Data: torch.Size([10, 384])	 Training Loss: 0.9928671717643738
Step: 134	 Data: torch.Size([10, 384])	 Training Loss: 1.0587447881698608
Step: 135	 Data: torch.Size([10, 384])	 Training Loss: 0.9774086475372314
Step: 136	 Data: torch.Size([10, 384])	 Training Loss: 0.9293026924133301
Step: 137	 Data: torch.Size([10, 384])	 Training Loss: 1.054977536201477
Step: 138	 Data: torch.Size([10, 384])	 Training Loss: 0.9142991900444031
Step: 139	 Data: torch.Size([10, 384])	 Training Loss: 1.0005671977996826
Step: 140	 Data: torch.Size([10, 384])	 Training Loss: 0.9571601748466492
Step: 141	 Data: torch.Size([10, 384])	 Training Loss: 0.9847841858863831
Step: 142	 Data: torch.Size([10, 384])	 Training Loss: 0.9687278866767883
Step: 143	 Data: torch.Size([10, 384])	 Training Loss: 1.0087746381759644
Step: 144	 Data: torch.Size([10, 384])	 Training Loss: 1.0080183744430542
Step: 145	 Data: torch.Size([10, 384])	 Training Loss: 0.9466089606285095
Step: 146	 Data: torch.Size([10, 384])	 Training Loss: 1.0043705701828003
Step: 147	 Data: torch.Size([10, 384])	 Training Loss: 0.9840805530548096
Step: 148	 Data: torch.Size([10, 384])	 Training Loss: 0.9330374002456665
Step: 149	 Data: torch.Size([10, 384])	 Training Loss: 0.898991048336029
Step: 150	 Data: torch.Size([10, 384])	 Training Loss: 0.9673881530761719
Step: 151	 Data: torch.Size([10, 384])	 Training Loss: 1.0541800260543823
Step: 152	 Data: torch.Size([10, 384])	 Training Loss: 0.8943882584571838
Step: 153	 Data: torch.Size([10, 384])	 Training Loss: 1.110238790512085
Step: 154	 Data: torch.Size([10, 384])	 Training Loss: 1.0239907503128052
Step: 155	 Data: torch.Size([10, 384])	 Training Loss: 0.930576741695404
Step: 156	 Data: torch.Size([10, 384])	 Training Loss: 0.8394049406051636
Step: 157	 Data: torch.Size([10, 384])	 Training Loss: 0.8594812750816345
Step: 158	 Data: torch.Size([10, 384])	 Training Loss: 0.9799162745475769
Step: 159	 Data: torch.Size([10, 384])	 Training Loss: 1.054760217666626
Step: 160	 Data: torch.Size([10, 384])	 Training Loss: 0.8693251013755798
Step: 161	 Data: torch.Size([10, 384])	 Training Loss: 1.0345969200134277
Step: 162	 Data: torch.Size([10, 384])	 Training Loss: 1.0002458095550537
Step: 163	 Data: torch.Size([10, 384])	 Training Loss: 0.9832301139831543
Step: 164	 Data: torch.Size([10, 384])	 Training Loss: 0.9503297805786133
Step: 165	 Data: torch.Size([10, 384])	 Training Loss: 0.904559314250946
Step: 166	 Data: torch.Size([10, 384])	 Training Loss: 1.0807437896728516
Step: 167	 Data: torch.Size([10, 384])	 Training Loss: 0.923624575138092
Step: 168	 Data: torch.Size([10, 384])	 Training Loss: 1.0211108922958374
Step: 169	 Data: torch.Size([10, 384])	 Training Loss: 0.9641257524490356
Step: 170	 Data: torch.Size([10, 384])	 Training Loss: 1.0714571475982666
Step: 171	 Data: torch.Size([10, 384])	 Training Loss: 0.9109988808631897
Step: 172	 Data: torch.Size([10, 384])	 Training Loss: 0.8482170104980469
Step: 173	 Data: torch.Size([10, 384])	 Training Loss: 0.9153372049331665
Step: 174	 Data: torch.Size([10, 384])	 Training Loss: 0.9033724665641785
Step: 175	 Data: torch.Size([10, 384])	 Training Loss: 1.037381887435913
Step: 176	 Data: torch.Size([10, 384])	 Training Loss: 0.951526403427124
Step: 177	 Data: torch.Size([10, 384])	 Training Loss: 0.9924489259719849
Step: 178	 Data: torch.Size([10, 384])	 Training Loss: 0.9001294374465942
Step: 179	 Data: torch.Size([10, 384])	 Training Loss: 0.836117148399353
Step: 180	 Data: torch.Size([10, 384])	 Training Loss: 1.0478261709213257
Step: 181	 Data: torch.Size([10, 384])	 Training Loss: 0.7372348308563232
Step: 182	 Data: torch.Size([10, 384])	 Training Loss: 1.0811429023742676
Step: 183	 Data: torch.Size([10, 384])	 Training Loss: 0.8644870519638062
Step: 184	 Data: torch.Size([10, 384])	 Training Loss: 0.9487614631652832
Step: 185	 Data: torch.Size([10, 384])	 Training Loss: 0.8512436747550964
Step: 186	 Data: torch.Size([10, 384])	 Training Loss: 0.9864374399185181
Step: 187	 Data: torch.Size([10, 384])	 Training Loss: 0.8675556182861328
Step: 188	 Data: torch.Size([10, 384])	 Training Loss: 0.968634307384491
Step: 189	 Data: torch.Size([10, 384])	 Training Loss: 0.9290087223052979
Step: 190	 Data: torch.Size([10, 384])	 Training Loss: 1.098487377166748
Step: 191	 Data: torch.Size([10, 384])	 Training Loss: 0.8907138705253601
Step: 192	 Data: torch.Size([10, 384])	 Training Loss: 1.0063965320587158
Step: 193	 Data: torch.Size([10, 384])	 Training Loss: 0.9360946416854858
Step: 194	 Data: torch.Size([10, 384])	 Training Loss: 0.8809037208557129
Step: 195	 Data: torch.Size([10, 384])	 Training Loss: 0.9393028020858765
Step: 196	 Data: torch.Size([10, 384])	 Training Loss: 0.9158994555473328
Step: 197	 Data: torch.Size([10, 384])	 Training Loss: 0.8391836881637573
Step: 198	 Data: torch.Size([10, 384])	 Training Loss: 1.0164427757263184
Step: 199	 Data: torch.Size([10, 384])	 Training Loss: 1.0054064989089966
Step: 200	 Data: torch.Size([10, 384])	 Training Loss: 0.9031193852424622
Step: 201	 Data: torch.Size([10, 384])	 Training Loss: 0.9703789353370667
Step: 202	 Data: torch.Size([10, 384])	 Training Loss: 0.9000686407089233
Step: 203	 Data: torch.Size([10, 384])	 Training Loss: 1.0076558589935303
Step: 204	 Data: torch.Size([10, 384])	 Training Loss: 0.9769771099090576
Step: 205	 Data: torch.Size([10, 384])	 Training Loss: 0.9961204528808594
Step: 206	 Data: torch.Size([10, 384])	 Training Loss: 0.8991567492485046
Step: 207	 Data: torch.Size([10, 384])	 Training Loss: 0.8742285370826721
Step: 208	 Data: torch.Size([10, 384])	 Training Loss: 0.9839516878128052
Step: 209	 Data: torch.Size([10, 384])	 Training Loss: 0.9528692960739136
Step: 210	 Data: torch.Size([10, 384])	 Training Loss: 0.9883854985237122
Step: 211	 Data: torch.Size([10, 384])	 Training Loss: 0.8949360847473145
Step: 212	 Data: torch.Size([10, 384])	 Training Loss: 0.8836813569068909
Step: 213	 Data: torch.Size([10, 384])	 Training Loss: 0.929034948348999
Step: 214	 Data: torch.Size([10, 384])	 Training Loss: 1.0802083015441895
Step: 215	 Data: torch.Size([10, 384])	 Training Loss: 0.9051576256752014
Step: 216	 Data: torch.Size([10, 384])	 Training Loss: 0.9154974818229675
Step: 217	 Data: torch.Size([10, 384])	 Training Loss: 1.0370227098464966
Step: 218	 Data: torch.Size([10, 384])	 Training Loss: 0.9826076030731201
Step: 219	 Data: torch.Size([10, 384])	 Training Loss: 1.044632911682129
Step: 220	 Data: torch.Size([10, 384])	 Training Loss: 0.8752811551094055
Step: 221	 Data: torch.Size([10, 384])	 Training Loss: 1.0104048252105713
Step: 222	 Data: torch.Size([10, 384])	 Training Loss: 0.8537352681159973
Step: 223	 Data: torch.Size([10, 384])	 Training Loss: 0.9310157895088196
Step: 224	 Data: torch.Size([10, 384])	 Training Loss: 0.9313865303993225
Step: 225	 Data: torch.Size([10, 384])	 Training Loss: 0.8146920204162598
Step: 226	 Data: torch.Size([10, 384])	 Training Loss: 0.8580584526062012
Step: 227	 Data: torch.Size([10, 384])	 Training Loss: 0.9831962585449219
Step: 228	 Data: torch.Size([10, 384])	 Training Loss: 0.9073008894920349
Step: 229	 Data: torch.Size([10, 384])	 Training Loss: 0.8960647583007812
Step: 230	 Data: torch.Size([10, 384])	 Training Loss: 0.8235786557197571
Step: 231	 Data: torch.Size([10, 384])	 Training Loss: 0.944125235080719
Step: 232	 Data: torch.Size([10, 384])	 Training Loss: 0.984612762928009
Step: 233	 Data: torch.Size([10, 384])	 Training Loss: 0.973260760307312
Step: 234	 Data: torch.Size([10, 384])	 Training Loss: 0.8214860558509827
Step: 235	 Data: torch.Size([10, 384])	 Training Loss: 0.8219550848007202
Step: 236	 Data: torch.Size([10, 384])	 Training Loss: 0.9459847807884216
Step: 237	 Data: torch.Size([10, 384])	 Training Loss: 1.023280382156372
Step: 238	 Data: torch.Size([10, 384])	 Training Loss: 1.0353552103042603
Step: 239	 Data: torch.Size([10, 384])	 Training Loss: 0.9317419528961182
Step: 240	 Data: torch.Size([10, 384])	 Training Loss: 0.946476399898529
Step: 241	 Data: torch.Size([10, 384])	 Training Loss: 1.0208654403686523
Step: 242	 Data: torch.Size([10, 384])	 Training Loss: 0.9062461256980896
Step: 243	 Data: torch.Size([10, 384])	 Training Loss: 1.061453104019165
Step: 244	 Data: torch.Size([10, 384])	 Training Loss: 1.0029504299163818
Step: 245	 Data: torch.Size([10, 384])	 Training Loss: 0.9577422738075256
Step: 246	 Data: torch.Size([10, 384])	 Training Loss: 1.0228948593139648
Step: 247	 Data: torch.Size([10, 384])	 Training Loss: 0.8967358469963074
Step: 248	 Data: torch.Size([10, 384])	 Training Loss: 0.8274109959602356
Step: 249	 Data: torch.Size([10, 384])	 Training Loss: 0.8980391621589661
Step: 250	 Data: torch.Size([10, 384])	 Training Loss: 1.0335291624069214
Step: 251	 Data: torch.Size([10, 384])	 Training Loss: 0.997875452041626
Step: 252	 Data: torch.Size([10, 384])	 Training Loss: 0.9952898621559143
Step: 253	 Data: torch.Size([10, 384])	 Training Loss: 0.9722899198532104
Step: 254	 Data: torch.Size([10, 384])	 Training Loss: 0.9307318329811096
Step: 255	 Data: torch.Size([10, 384])	 Training Loss: 0.8538421392440796
Step: 256	 Data: torch.Size([10, 384])	 Training Loss: 0.9453589916229248
Step: 257	 Data: torch.Size([10, 384])	 Training Loss: 0.8983491659164429
Step: 258	 Data: torch.Size([10, 384])	 Training Loss: 0.8978544473648071
Step: 259	 Data: torch.Size([10, 384])	 Training Loss: 0.8397630453109741
Step: 260	 Data: torch.Size([10, 384])	 Training Loss: 0.8955990076065063
Step: 261	 Data: torch.Size([10, 384])	 Training Loss: 1.0282094478607178
Step: 262	 Data: torch.Size([10, 384])	 Training Loss: 0.9749388098716736
Step: 263	 Data: torch.Size([10, 384])	 Training Loss: 1.048328161239624
Step: 264	 Data: torch.Size([10, 384])	 Training Loss: 1.0506633520126343
Step: 265	 Data: torch.Size([10, 384])	 Training Loss: 0.9406599402427673
Step: 266	 Data: torch.Size([10, 384])	 Training Loss: 0.8841466903686523
Step: 267	 Data: torch.Size([10, 384])	 Training Loss: 0.8793704509735107
Step: 268	 Data: torch.Size([10, 384])	 Training Loss: 0.9010610580444336
Step: 269	 Data: torch.Size([10, 384])	 Training Loss: 0.8832207322120667
Step: 270	 Data: torch.Size([10, 384])	 Training Loss: 0.9878657460212708
Step: 271	 Data: torch.Size([10, 384])	 Training Loss: 0.952043354511261
Step: 272	 Data: torch.Size([10, 384])	 Training Loss: 0.906556248664856
Step: 273	 Data: torch.Size([10, 384])	 Training Loss: 0.8201808333396912
Step: 274	 Data: torch.Size([10, 384])	 Training Loss: 0.9700203537940979
Step: 275	 Data: torch.Size([10, 384])	 Training Loss: 1.007697582244873
Step: 276	 Data: torch.Size([10, 384])	 Training Loss: 0.9407210350036621
Step: 277	 Data: torch.Size([10, 384])	 Training Loss: 0.9095225930213928
Step: 278	 Data: torch.Size([10, 384])	 Training Loss: 1.02153742313385
Step: 279	 Data: torch.Size([10, 384])	 Training Loss: 0.9734605550765991
Step: 280	 Data: torch.Size([10, 384])	 Training Loss: 0.9650838375091553
Step: 281	 Data: torch.Size([10, 384])	 Training Loss: 0.9942392110824585
Step: 282	 Data: torch.Size([10, 384])	 Training Loss: 0.9297943115234375
Step: 283	 Data: torch.Size([10, 384])	 Training Loss: 1.0491777658462524
Step: 284	 Data: torch.Size([10, 384])	 Training Loss: 1.2717896699905396
Step: 285	 Data: torch.Size([10, 384])	 Training Loss: 0.9261228442192078
Step: 286	 Data: torch.Size([10, 384])	 Training Loss: 1.0581011772155762
Step: 287	 Data: torch.Size([10, 384])	 Training Loss: 0.9081796407699585
Step: 288	 Data: torch.Size([10, 384])	 Training Loss: 0.7755696773529053
Step: 289	 Data: torch.Size([10, 384])	 Training Loss: 0.9319201111793518
Step: 290	 Data: torch.Size([10, 384])	 Training Loss: 0.9150975346565247
Step: 291	 Data: torch.Size([10, 384])	 Training Loss: 0.9473457336425781
Step: 292	 Data: torch.Size([10, 384])	 Training Loss: 1.0281018018722534
Step: 293	 Data: torch.Size([10, 384])	 Training Loss: 0.8550304174423218
Step: 294	 Data: torch.Size([10, 384])	 Training Loss: 0.8533535599708557
Step: 295	 Data: torch.Size([10, 384])	 Training Loss: 0.985013484954834
Step: 296	 Data: torch.Size([10, 384])	 Training Loss: 1.0069568157196045
Step: 297	 Data: torch.Size([10, 384])	 Training Loss: 0.940868616104126
Step: 298	 Data: torch.Size([10, 384])	 Training Loss: 0.9391941428184509
Step: 299	 Data: torch.Size([10, 384])	 Training Loss: 1.0083125829696655
Step: 300	 Data: torch.Size([10, 384])	 Training Loss: 1.0460517406463623
