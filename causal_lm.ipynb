{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果语言模型训练实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk(\"./wiki_cn_filtered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'wikipedia.zh2307',\n",
       " 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genghaozhe/.pyenv/versions/3.10.14/envs/colossalai-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]]\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1022, 11396,  3347,  1813,  1504,  6566,  1813,   355,  9155,  8633,\n",
      "          1504,  2063,  1813,   189,    13, 23158,   813,  7817,  5358,     2],\n",
      "        [  124,   168,   117,   228,  6279,   100,   124,   168,   117,   228,\n",
      "           171,   238,   224, 41356,   236, 24175, 11082, 10981, 21350,  9067]])\n",
      "tensor([[ 1022, 11396,  3347,  1813,  1504,  6566,  1813,   355,  9155,  8633,\n",
      "          1504,  2063,  1813,   189,    13, 23158,   813,  7817,  5358,     2],\n",
      "        [  124,   168,   117,   228,  6279,   100,   124,   168,   117,   228,\n",
      "           171,   238,   224, 41356,   236, 24175, 11082, 10981, 21350,  9067]])\n"
     ]
    }
   ],
   "source": [
    "i, data = next(enumerate(dl))\n",
    "print(data['input_ids'][:,-20:])\n",
    "print(data['labels'][:,-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(42437, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=42437, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "lables: torch.Size([2, 384])\n",
      "logits: torch.Size([2, 384, 42437])\n",
      "shift_labels: torch.Size([2, 383])\n",
      "shift_logits: torch.Size([2, 383, 42437])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384, 42437])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, inp = next(enumerate(dl))\n",
    "print(inp['input_ids'].shape)\n",
    "output = model(**inp)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 383, 42437])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[..., :-1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([766, 42437])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = output.logits[..., :-1, :].contiguous()\n",
    "batch_size, seq_length, vocab_size = shift_logits.shape\n",
    "shift_logits.view(batch_size * seq_length, vocab_size).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([766])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.randn(2*384).reshape(2, 384)[..., 1:].contiguous().view(batch_size * seq_length).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./causal_lm\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee7c613facf4dcb8efa79d39e0618fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.985, 'learning_rate': 4.83974358974359e-05, 'epoch': 0.03}\n",
      "{'loss': 3.9901, 'learning_rate': 4.67948717948718e-05, 'epoch': 0.06}\n",
      "{'loss': 3.8418, 'learning_rate': 4.519230769230769e-05, 'epoch': 0.1}\n",
      "{'loss': 3.8249, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.13}\n",
      "{'loss': 3.6815, 'learning_rate': 4.198717948717949e-05, 'epoch': 0.16}\n",
      "{'loss': 3.6652, 'learning_rate': 4.038461538461539e-05, 'epoch': 0.19}\n",
      "{'loss': 3.6319, 'learning_rate': 3.878205128205129e-05, 'epoch': 0.22}\n",
      "{'loss': 3.6918, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.26}\n",
      "{'loss': 3.6513, 'learning_rate': 3.557692307692308e-05, 'epoch': 0.29}\n",
      "{'loss': 3.6396, 'learning_rate': 3.397435897435898e-05, 'epoch': 0.32}\n",
      "{'loss': 3.5632, 'learning_rate': 3.2371794871794876e-05, 'epoch': 0.35}\n",
      "{'loss': 3.5992, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.38}\n",
      "{'loss': 3.6086, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.42}\n",
      "{'loss': 3.5191, 'learning_rate': 2.756410256410257e-05, 'epoch': 0.45}\n",
      "{'loss': 3.5824, 'learning_rate': 2.5961538461538464e-05, 'epoch': 0.48}\n",
      "{'loss': 3.574, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.51}\n",
      "{'loss': 3.546, 'learning_rate': 2.2756410256410258e-05, 'epoch': 0.54}\n",
      "{'loss': 3.5498, 'learning_rate': 2.1153846153846154e-05, 'epoch': 0.58}\n",
      "{'loss': 3.4933, 'learning_rate': 1.9551282051282052e-05, 'epoch': 0.61}\n",
      "{'loss': 3.4487, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.64}\n",
      "{'loss': 3.4897, 'learning_rate': 1.6346153846153847e-05, 'epoch': 0.67}\n",
      "{'loss': 3.5171, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.7}\n",
      "{'loss': 3.5087, 'learning_rate': 1.3141025641025642e-05, 'epoch': 0.74}\n",
      "{'loss': 3.4034, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.77}\n",
      "{'loss': 3.4573, 'learning_rate': 9.935897435897435e-06, 'epoch': 0.8}\n",
      "{'loss': 3.4651, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.83}\n",
      "{'loss': 3.4208, 'learning_rate': 6.730769230769231e-06, 'epoch': 0.86}\n",
      "{'loss': 3.429, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.9}\n",
      "{'loss': 3.5004, 'learning_rate': 3.525641025641026e-06, 'epoch': 0.93}\n",
      "{'loss': 3.5131, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.96}\n",
      "{'loss': 3.4888, 'learning_rate': 3.205128205128205e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 374.5245, 'train_samples_per_second': 26.701, 'train_steps_per_second': 0.833, 'train_loss': 3.58796650935442, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=3.58796650935442, metrics={'train_runtime': 374.5245, 'train_samples_per_second': 26.701, 'train_steps_per_second': 0.833, 'train_loss': 3.58796650935442, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学西安学院西楼三期大楼的二层混合式博物馆。该建筑占地约2000平方米，于2012年落成开馆，是西安交通大学博物馆的一个组成部分，由西安交通大学及陕西省西安市设计与艺术设计研究院联合发起筹建。该建筑是西安交通大学学生宿舍（西安理工大学建筑学院宿舍）及学生食堂（西安理工大学建筑学院食堂）的一部份，建成后将极大地方便西安交通大学所有学生的住宿和出行。\\n博物馆馆舍\\n博物馆位于西安交通大学西安学院西楼3号楼，由陕西西安设计院\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常之盛。而随着这几年游戏的高速发展，玩家对于游戏的了解越来越强烈，以至于在游戏行业里，“烧钱”或“低成本”的定义也逐渐淡出了人们的视线。\\n虽然，这是指游戏产业，但这仅仅是以游戏厂商，而不是游戏开发商为准。\\n娱乐产业\\n近期，随着游戏产业的发展，大量有潜力的电子游戏公司，如电子游戏工作室Game Factory已经或正在开发大型商业化游戏，并且已经确定了。这些游戏产业的公司，有的是以研发游戏，而有一部分，如N'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
